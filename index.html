<!DOCTYPE html>
<html>
    <head>
        <title>demo</title>
        <style>
            body {
                margin-left: 20px;
                margin-bottom: 30px;
            }
            div.menu {
                box-sizing: 100%;
                height: 10px;
                padding: 20px;
                border: 3px solid #000000;
                border-radius: 10px;
                border-style: inset;
                background-color: #ffffe6;
                opacity: 0.7;
                a:hover {
                    background-color: #FFFF4D;
                    font-weight: bold;
                }
                a {
                    float: right;
                    padding: 0px 20px;
                }
            }
            div#titel {
                text-align: center;
            }
            #ondertitel { 
                text-indent: 3em;
            }
            article {
                margin: 30px 75px 75px 75px;
            }
            #fig1 {
                margin-left:50px;
            }
            #fig2 {
                text-indent: -2em;
            }
            h1,h4 {
                line-height: 50%;
            }
            h3 {
                line-height: 10%;
            }
        </style>
    </head>       
    <body>
        <div class="menu">
            <!--<a href="index.html">Technical Writer demo</a>-->
            <b style="color:#0000FF">Technical Writer demo</b>
            &emsp;&emsp;<a href="https://play.unity.com/en/user/614517bb-c61e-4d73-aee7-c0fe82c1a588">Unity portfolio</a>&emsp;
            <a href="https://www.linkedin.com/in/caritar/">Linkedin profile</a>&emsp;
        </div>

        <br>

        <article id="alertmon">
            <div class="titel">
                <h1>The life of a monitoring alert</h1>
                <h4 id="ondertitel">- how a monitoring alert travels through the path to resolution</h3>
            </div>
        
        <br>

        <section class="intro">
            <p>
                The following describes the origin or creation of a monitoring alert and follows its path through resolution, while highlighting the various teams in Operations and their actions, that are involved in this process. 
            </p>
        </section>

        <br>

        <section class="sect">
            <h3>-- Operations teams --</h3>
            <p>In this description, the term &apos;Operations&apos; will be restricted to: the Operations Center (OC) team as Tier1 Support, the Service Reliability Engineer (SRE) teams as Tier2 Support, Development-and-Operations (DevOps) teams as Tier3 Support, and the Incident Management &amp; Problem Management teams.
            </p>
        </section>

        <br>

        <section class="sect">
            <h3>-- alert creation --</h3>
            <p>Systems are ideally built to keep running efficiently and with as little downtime as possible;
to accomplish this, a monitoring tool is implemented: a watchdog to notify the keepers of the system when something is off or broken, otherwise known as “failed”.
To figure out when something (an ‘object’) is in the process of failing or has failed, SRE and/or DevOps teams determine threshold values; these represent the value at which the specific object will be outside ('alert' when failing) or back inside ('clear' when recovering) the acceptable range of operating values. 
Next, they setup event triggers: whenever a specific object in the system has reached a value above or below a preset threshold value, an ‘event’ signal is sent out.
            </p>

            <figure id="fig1">
                <img src="alert_triggers.png" alt="event triggers">
                <figcaption><i><b>&nbsp;&nbsp;&nbsp;Figure 1  - </b> event triggers</i></figcaption>
            </figure>
            
            <p>For example, see Figure 1: disk space availability needs to be monitored, so a ‘high threshold’ value of  “85% full” is setup, as well as a ‘low/clear threshold’ value of “50% full”. When a hard drive (object) is more than 85% full, an <i>'alert'</i> event signal would be sent;
more disk space will then be made available, either manually or via script, until the lower limit of 50% of disk space available has been reached – then a <i>'clear'</i> (‘alert cleared’) event signal is sent out.
            </p>

            <p> ‘Agents’, placed in strategic locations within the system, collect incoming events. Based on a set of pre-programmed rules that determine whether an event is noteworthy or not, these incoming event signals are then ignored / dropped, or forwarded on individually or grouped with similar events, as an ‘alert’.</p>

            <p>As incoming alerts from all agents are aggregated into a central collection point of the monitoring tool application, yet more alert processing rules - a combination of built-in and user-defined alert processing rules – are applied, such as further grouping alerts or dropping alerts; then tagging is added for alert display, such as alert origination details (e.g. system name/ location/time), priority levels and their priority-related colors, ranging from “very urgent” (red) to “needs further investigation”, or “cleared” (green). </p>

            <figure id="fig2">
                <img src="event-to-alert1.png" alt="event to alert diagram">
                <figcaption><i><b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Figure 2  - </b> alert path through monitoring tool(s)</i></figcaption>
            </figure>

        </section>
        
        <br>

        <section  class="sect">
            <h3>-- alert notification & display --</h3>
            <p>Alert notifications can be sent directly from the triggering mechanism as emails, or via notification applications (such as Opsgenie) as phone calls, text messages or alert messages. Alerts can also be pushed to a monitoring tool ‘Alert dashboard’, where the Operations Center will act on the incoming alerts and convert the unresolved / critical ones to Incidents to be escalated to the next Support Tier or Incident Management, depending on priority and impact level of the alert.
  For example: an incoming P2 alert with  a ‘high’ impact would be handed off to the Incident Management team, whereas an incoming P3 alert with ‘medium’ impact and ‘medium urgency would be sent to the SRE team (Tier2).
</p>

            <p>Priority levels of alerts can be assigned between 1 and 5, where a priority 1 (P1) is of the highest level and priority 5 (P5) is the least urgent or impactful:</p>  
            
            <figure id="fig3">
                <img src="priority-levels.png" alt="priority levels list">
                <figcaption><i><b>Figure 3  - </b> Priority levels</i></figcaption>
            </figure>

            <p>A priority level 0 (zero) can be reserved for extremely urgent and business-critical impact, that requires immediate action with CEO-level involvement.
The combination of urgency and impact of an alert determines its priority level, as illustrated in the diagram below:
</p>
            
            <figure id="fig4">
                <img src="impact-urgency-matrix.png" alt="impact-urgency matrix">
                <figcaption><i><b>Figure 4  - </b> Impact, Urgency & Priority matrix</i></figcaption>
            </figure>

            <p>  For example: an alert that has a low impact on the system and/or users, and has a medium-level of urgency to fix, is assigned priority level 4 (P4 - low)</p>

            <p>Priority levels are then tied to how much time is allocated to resolve the alert(s) that caused the incident; this is done via Service Level Agreements (SLAs) and SLOs (Service Level Objectives). Through an SLA, the Operations teams provide pre-determined response times and alert resolution times for specific priority levels.
  For example: in the SLA, it can be specified that any P2 alerts will be responded to within 10 minutes, and aim to be stabilized within 30 minutes
</p>

        </section>   
        
        <br>

        <section class="sect">
            <h3>-- incident & resolution --</h3>
            <p>When a P3-P5 incident is created from an alert, depending on its type and priority level, the NOC can either resolve it themselves or escalate it to the next level (SRE); when received by the SRE teams, this incident can either be resolved or escalated again to the DevOps team.
The DevOps team is the ultimate stop, where the incident will be resolved by tuning the alerting application / system, or it could be snoozed for further monitoring and future review leading to application / system upgrades, patches or rollbacks (see figure 5).</p>
            
            <figure id="fig5">
                <img src="non_User_Impacting_Incident_Flow1.png" alt="non-User-Impacting Incident Flow">
                <figcaption><i><b>&nbsp;&nbsp;&nbsp;Figure 5  - </b> non-User-Impacting  incident flow</i></figcaption>
            </figure>

            <p>Incoming P0-P2 incidents are transferred from the NOC team to the Incident Management team, to create a group chat / call collaboration between the SRE and DevOps team members that are currently oncall to immediately start troubleshooting the issue, while the Incident Management team also loops in the pre-arranged executive team members with timed updates.
While the incident is ongoing, an Incident Management team member (Incident Manager) manages the communications between all parties -including regular request for status updates as the SRE and DevOps teams are troubleshooting- , updates executives, and manages a shared information spreadsheet for listing affected components, their individual statuses,  and any resolutions and/or comments.
            </p>

            <figure id="fig6">
                <img src="User_Impacting_Incident_Flow.png" alt="User-Impacting Incident Flow">
                <figcaption><i><b>&nbsp;&nbsp;Figure 6  - </b> User-Impacting (UI) incident flow</i></figcaption>
            </figure>

            <p>When the incident is stabilized (meaning: a work-around or patch has been implemented, so that the system can keep running, however in a less-than-optimal state) or fully resolved, the Incident Manager then gathers many statistics  about the incident, including the root cause determined by  the SRE / DevOps teams, how many users / customers were impacted, how much time the incident took from initiation to resolution / stabilization, SRE / DevOps team response times to oncall requests to join the incident, how much time expired between the alert detection and when users / customers were impacted, and much more (see figure 6). 
            </p>

        </section> 

        <br>

        <section class="sect">
            <h3>-- alert / incident prevention --</h3>
            <p>The Incident Manager then closes the incident and passes the data on to the Problem Management team, which sets up a post-mortem (incident review) meeting that includes at minimum the SRE / DevOps team members that were involved in resolving the incident; an official root cause is established / confirmed and any possible Remediations (actions to prevent similar incidents) and Changes (configuration changes to the existing system) determined, which are then completed within an ageed-upon time frame.</p>
        </section> 


            
        </article>



    </body>
</html>